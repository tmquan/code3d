{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points\n",
    "from pytorch3d.renderer import (\n",
    "    AbsorptionOnlyRaymarcher,\n",
    "    AlphaCompositor,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    MonteCarloRaysampler,\n",
    "    MultinomialRaysampler,\n",
    "    NDCMultinomialRaysampler,\n",
    "    PerspectiveCameras,\n",
    "    RayBundle,\n",
    "    VolumeRenderer,\n",
    "    VolumeSampler,\n",
    ")\n",
    "\n",
    "from pytorch3d.structures.volumes import Volumes\n",
    "from pytorch3d.transforms.so3 import so3_exp_map\n",
    "from pytorch3d.vis.plotly_vis import plot_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_TRANSLATION = torch.zeros(1, 3)\n",
    "\n",
    "def init_uniform_y_rotations(batch_size: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Generate a batch of `batch_size` 3x3 rotation matrices around y-axis\n",
    "    whose angles are uniformly distributed between 0 and 2 pi.\n",
    "    \"\"\"\n",
    "    axis = torch.tensor([0.0, 1.0, 0.0], device=device, dtype=torch.float32)\n",
    "    angles = torch.linspace(0, 2.0 * np.pi, batch_size + 1, device=device)\n",
    "    angles = angles[:batch_size]\n",
    "    log_rots = axis[None, :] * angles[:, None]\n",
    "    R = so3_exp_map(log_rots)\n",
    "    return R\n",
    "    \n",
    "def init_boundary_volume(\n",
    "    batch_size: int,\n",
    "    volume_size: Tuple[int, int, int],\n",
    "    border_offset: int = 2,\n",
    "    shape: str = \"cube\",\n",
    "    volume_translation: torch.Tensor = ZERO_TRANSLATION,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a volume with sides colored with distinct colors.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    # first center the volume for the purpose of generating the canonical shape\n",
    "    volume_translation_tmp = (0.0, 0.0, 0.0)\n",
    "\n",
    "    # set the voxel size to 1 / (volume_size-1)\n",
    "    volume_voxel_size = 1 / (volume_size[0] - 1.0)\n",
    "\n",
    "    # colors of the sides of the cube\n",
    "    clr_sides = torch.tensor(\n",
    "        [\n",
    "            [1.0, 1.0, 1.0],\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [1.0, 0.0, 1.0],\n",
    "            [1.0, 1.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 1.0, 1.0],\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # get the coord grid of the volume\n",
    "    coord_grid = Volumes(\n",
    "        densities=torch.zeros(1, 1, *volume_size, device=device),\n",
    "        voxel_size=volume_voxel_size,\n",
    "        volume_translation=volume_translation_tmp,\n",
    "    ).get_coord_grid()[0]\n",
    "\n",
    "    # extract the boundary points and their colors of the cube\n",
    "    if shape == \"cube\":\n",
    "        boundary_points, boundary_colors = [], []\n",
    "        for side, clr_side in enumerate(clr_sides):\n",
    "            first = side % 2\n",
    "            dim = side // 2\n",
    "            slices = [slice(border_offset, -border_offset, 1)] * 3\n",
    "            slices[dim] = int(border_offset * (2 * first - 1))\n",
    "            slices.append(slice(0, 3, 1))\n",
    "            boundary_points_ = coord_grid[slices].reshape(-1, 3)\n",
    "            boundary_points.append(boundary_points_)\n",
    "            boundary_colors.append(clr_side[None].expand_as(boundary_points_))\n",
    "        # set the internal part of the volume to be completely opaque\n",
    "        volume_densities = torch.zeros(*volume_size, device=device)\n",
    "        volume_densities[[slice(border_offset, -border_offset, 1)] * 3] = 0.01\n",
    "        boundary_points, boundary_colors = [\n",
    "            torch.cat(p, dim=0) for p in [boundary_points, boundary_colors]\n",
    "        ]\n",
    "        # color the volume voxels with the nearest boundary points' color\n",
    "        _, idx, _ = knn_points(\n",
    "            coord_grid.view(1, -1, 3), boundary_points.view(1, -1, 3)\n",
    "        )\n",
    "        volume_colors = (\n",
    "            boundary_colors[idx.view(-1)].view(*volume_size, 3).permute(3, 0, 1, 2)\n",
    "        )\n",
    "\n",
    "    elif shape == \"sphere\":\n",
    "        # set all voxels within a certain distance from the origin to be opaque\n",
    "        volume_densities = (\n",
    "            coord_grid.norm(dim=-1)\n",
    "            <= 0.5 * volume_voxel_size * (volume_size[0] - border_offset)\n",
    "        ).float() / 50.\n",
    "        # color each voxel with the standrd spherical color\n",
    "        volume_colors = (\n",
    "            (torch.nn.functional.normalize(coord_grid, dim=-1) + 1.0) * 0.5\n",
    "        ).permute(3, 0, 1, 2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(shape)\n",
    "\n",
    "    volume_voxel_size = torch.ones((batch_size, 1), device=device) * volume_voxel_size\n",
    "    volume_translation = volume_translation.expand(batch_size, 3)\n",
    "    volumes = Volumes(\n",
    "        densities=volume_densities[None, None].expand(batch_size, 1, *volume_size),\n",
    "        features=volume_colors[None].expand(batch_size, 3, *volume_size),\n",
    "        voxel_size=volume_voxel_size,\n",
    "        volume_translation=volume_translation,\n",
    "    )\n",
    "\n",
    "    return volumes, volume_voxel_size, volume_translation\n",
    "\n",
    "def init_cameras(\n",
    "    batch_size: int = 10,\n",
    "    image_size: Optional[Tuple[int, int]] = (50, 50),\n",
    "    ndc: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize a batch of cameras whose extrinsics rotate the cameras around\n",
    "    the world's y axis.\n",
    "    Depending on whether we want an NDC-space (`ndc==True`) or a screen-space camera,\n",
    "    the camera's focal length and principal point are initialized accordingly:\n",
    "        For `ndc==False`, p0=focal_length=image_size/2.\n",
    "        For `ndc==True`, focal_length=1.0, p0 = 0.0.\n",
    "    The the z-coordinate of the translation vector of each camera is fixed to 1.5.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    # trivial rotations\n",
    "    R = init_uniform_y_rotations(batch_size=batch_size, device=device)\n",
    "\n",
    "    # move camera 1.5 m away from the scene center\n",
    "    T = torch.zeros((batch_size, 3), device=device)\n",
    "    T[:, 2] = 1.5\n",
    "\n",
    "    if ndc:\n",
    "        p0 = torch.zeros(batch_size, 2, device=device)\n",
    "        focal = torch.ones(batch_size, device=device)\n",
    "    else:\n",
    "        p0 = torch.ones(batch_size, 2, device=device)\n",
    "        p0[:, 0] *= image_size[1] * 0.5\n",
    "        p0[:, 1] *= image_size[0] * 0.5\n",
    "        focal = max(*image_size) * torch.ones(batch_size, device=device)\n",
    "\n",
    "    # convert to a Camera object\n",
    "    cameras = PerspectiveCameras(focal, p0, R=R, T=T, device=device)\n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128, 128, 4])\n"
     ]
    }
   ],
   "source": [
    "volume_size = [64, 64, 64]\n",
    "batch_size = 20\n",
    "shape = \"cube\"\n",
    "volume_translation = torch.zeros(4, 3)\n",
    "volume_translation.requires_grad = True\n",
    "image_size=(128, 128)\n",
    "\n",
    "volumes = init_boundary_volume(\n",
    "                    volume_size=volume_size,\n",
    "                    batch_size=batch_size,\n",
    "                    shape=shape,\n",
    "                    # volume_translation=volume_translation,\n",
    "                )[0]\n",
    "\n",
    "cameras = init_cameras(batch_size, image_size=image_size)\n",
    "raysampler = MultinomialRaysampler(\n",
    "            min_x=0.5,\n",
    "            max_x=image_size[1] - 0.5,\n",
    "            min_y=0.5,\n",
    "            max_y=image_size[0] - 0.5,\n",
    "            image_width=image_size[1],\n",
    "            image_height=image_size[0],\n",
    "            n_pts_per_ray=256,\n",
    "            min_depth=0.5,\n",
    "            max_depth=2.0,\n",
    "        )\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "images_opacities, ray_bundle = \\\n",
    "    VolumeRenderer(\n",
    "                raysampler=raysampler,\n",
    "                raymarcher=raymarcher,\n",
    "                sample_mode=\"bilinear\",\n",
    "            )(cameras=cameras, volumes=volumes)\n",
    "\n",
    "print(images_opacities.shape)\n",
    "\n",
    "# split output to the alpha channel and rendered images\n",
    "images, opacities = images_opacities[..., :3], images_opacities[..., 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported /home/qtran/code3d/tmp/cube.gif\n"
     ]
    }
   ],
   "source": [
    "# export the gif\n",
    "n_frames=20 \n",
    "fps=5\n",
    "# outdir = tempfile.gettempdir() + \"/test_volume_renderer_gifs\"\n",
    "outdir = os.path.join(os.getcwd(), 'tmp')\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "frames = []\n",
    "for image, opacity in zip(images, opacities):\n",
    "    image_pil = Image.fromarray(\n",
    "        (\n",
    "            torch.cat(\n",
    "                (image, opacity[..., None].repeat(1, 1, 3)), dim=1\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            * 255.0\n",
    "        ).astype(np.uint8)\n",
    "    )\n",
    "    frames.append(image_pil)\n",
    "# outfile = os.path.join(outdir, f\"{shape}_{sample_mode}.gif\")\n",
    "outfile = os.path.join(outdir, f\"{shape}.gif\")\n",
    "frames[0].save(\n",
    "    outfile,\n",
    "    save_all=True,\n",
    "    append_images=frames[1:],\n",
    "    duration=n_frames // fps,\n",
    "    loop=0,\n",
    ")\n",
    "print(f\"exported {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51793541473485875990746c0443809d6bef33d584e2b99950cb1559c0880bb8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('pytorch3d': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
